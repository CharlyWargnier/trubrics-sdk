{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6c656-5936-4f9b-baf3-195aa59f2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trubrics rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2758ff6-626f-4a5e-8643-61b81144cefe",
   "metadata": {},
   "source": [
    "**In this tutorial of the [Titanic Use Case](https://www.kaggle.com/c/titanic), you will:**\n",
    "- Initialise a `DataContext` with ML datasets and metadata from the titanic use case\n",
    "- Build some out-of-the-box validations on a trained model and the `DataContext` with the `ModelValidator`\n",
    "- Build a custom validation\n",
    "- Save validations to a `Trubric`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249214ce-4183-480d-8bef-de29f8a0fd9a",
   "metadata": {},
   "source": [
    "## Load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich  # for pretty cell outputs\n",
    "\n",
    "from trubrics.example import get_titanic_data_and_model\n",
    "train_df, test_df, model = get_titanic_data_and_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d7ae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Init DataContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c07be-2aae-4e14-92f5-6f4e4dee914b",
   "metadata": {},
   "source": [
    "*The DataContext allows you to wrap all ML data assets into a single object that can be used in the ModelValidator and FeedbackCollector. Read more [here](https://trubrics.github.io/trubrics-sdk/data_context/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003e2c4-74bc-48f4-b766-2d1ccd8b5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.context import DataContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b70d3-6aff-4fa2-ba51-524ef4f25699",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context = DataContext(\n",
    "    name=\"my_first_dataset\",\n",
    "    version=0.1,\n",
    "    testing_data=test_df,\n",
    "    target=\"Survived\",\n",
    "    training_data=train_df,\n",
    "    minimum_functionality_data=test_df.head(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b8997-3020-40f8-84f3-0a32bf18477b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Init ModelValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792bed67-970b-45c4-af7d-c35196c4cadb",
   "metadata": {},
   "source": [
    "*The `ModelValidator` allows you to build out-of-the-box and custom validations around your model. Read more [here](https://trubrics.github.io/trubrics-sdk/models/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99bba3-065b-47bb-8858-7df580b9e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.validations import ModelValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991339f-3318-4741-b0a1-8ab05533e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validator = ModelValidator(\n",
    "    data=data_context,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c586484c-a49c-4e25-bb09-8df9008a8d95",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use the ModelValidator to build out-of-the-box validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62309c-2aa4-4d89-9a7f-fb16636ea925",
   "metadata": {},
   "source": [
    "*Out-of-the-box validations allow you to start validating your model with a couple of lines of code. See all validations [here](https://trubrics.github.io/trubrics-sdk/validations/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90323425-1677-432e-a3c2-950565282e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance = [\n",
    "    model_validator.validate_performance_against_threshold(metric=\"precision\", threshold=0.7),\n",
    "    model_validator.validate_performance_against_threshold(metric=\"f1\", threshold=0.75),\n",
    "]\n",
    "rich.print(performance[0].dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147da74-790a-4742-a9e8-8dac010c2876",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Other features of [performance validations](https://trubrics.github.io/trubrics-sdk/validations/#performance):\n",
    "- Integrate [custom metrics](https://trubrics.github.io/trubrics-sdk/metrics/#2-custom-scoring-functions) with any python function\n",
    "- Validate [performance on any split](https://trubrics.github.io/trubrics-sdk/metrics/#3-data-slicing-functions) of the data with any python slicing function, or validate two splits have the same performance\n",
    "- Validate performance between training & testing datasets (e.g. to avoid overfitting / underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0cc18-de1a-44ca-a48e-9f5bb518f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# metrics are computed once, and stored in the .performances attribute of the `ModelValidator` \n",
    "pd.DataFrame(model_validator.performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e007ff-9bf9-42c0-aa5d-0c7d5213923d",
   "metadata": {},
   "source": [
    "#### Other out of the box validations\n",
    "- [Minimum functionality](https://trubrics.github.io/trubrics-sdk/validations/#minimum-functionality) validations\n",
    "- [Feature importance](https://trubrics.github.io/trubrics-sdk/validations/#feature-importance) validations\n",
    "- [Inference time](https://trubrics.github.io/trubrics-sdk/validations/#inference-time) validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f59af-9cce-4017-89ad-e2b476285a13",
   "metadata": {},
   "source": [
    "## Build custom validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c00ea2-6703-4ee3-a11b-4e69ddafc0c0",
   "metadata": {},
   "source": [
    "*Custom validations allow to build validations around feedback, or for specific needs of your use case (e.g. data validations). Read more [here](https://trubrics.github.io/trubrics-sdk/custom_validations/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cbb552-8cfc-4d27-8783-d1ed2d9d4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile custom_validator.py\n",
    "from trubrics.context import DataContext\n",
    "from trubrics.validations import ModelValidator\n",
    "from trubrics.validations.validation_output import (\n",
    "    validation_output,\n",
    "    validation_output_type,\n",
    ")\n",
    "\n",
    "\n",
    "class CustomValidator(ModelValidator):\n",
    "    def __init__(self, data: DataContext, model, custom_scorers=None, slicing_functions=None):\n",
    "        super().__init__(data, model, custom_scorers, slicing_functions)\n",
    "\n",
    "    def _validate_master_age(self, age_limit_master) -> validation_output_type:\n",
    "        \"\"\"\n",
    "        Write your custom validation function here.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "            This method is separated from validate_performance_for_different_fares\n",
    "            to apply @validation_output and for unit testing.\n",
    "\n",
    "            The @validation_output decorator allows you to generate a Validation object,\n",
    "            and must be used to be able to save your validation as part of a Trubric.\n",
    "            This decorator requires you to return values with the same type as validation_output_type.\n",
    "        \"\"\"\n",
    "        master_df = self.tm.data.testing_data.loc[lambda df: df[\"Title\"] == \"Master\"]\n",
    "        errors_df = master_df.loc[lambda df: df[\"Age\"] >= age_limit_master]\n",
    "        return len(errors_df) == 0, {\"errors_df\": errors_df.to_dict()}\n",
    "\n",
    "    @validation_output\n",
    "    def validate_master_age(self, age_limit_master: int, severity=None):\n",
    "        \"\"\"Validate that passengers with the title \"master\" are younger than a certain age\n",
    "\n",
    "        Args:\n",
    "            age_limit_master: cut off value for master\n",
    "\n",
    "        Returns:\n",
    "            True for success, false otherwise. With a results dictionary giving dict of errors.\n",
    "        \"\"\"\n",
    "        return self._validate_master_age(age_limit_master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf1b64-6d66-44f5-955f-9fd75db8529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom_validator = CustomValidator(data=data_context, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf10c3-2299-4cbb-860b-33e7b87208f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom = [model_custom_validator.validate_master_age(age_limit_master=13, severity=\"warning\")]\n",
    "rich.print(custom[0].dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c45ce41-d3f0-4e98-b638-7fb0ff817971",
   "metadata": {},
   "source": [
    "## Save validations as a trubric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1404e-7900-44ab-94e2-4978e4c33d9a",
   "metadata": {},
   "source": [
    "*A `Trubric` is a checklist of validations, and represents the gold standard that your ML system must conform to. Read more about saving your validations as a `Trubric` [here](https://trubrics.github.io/trubrics-sdk/save_trubric/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bdeea-c7bc-4833-b89c-c184f056045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.validations import Trubric\n",
    "\n",
    "validations = performance + custom\n",
    "\n",
    "trubric = Trubric(\n",
    "    name=\"my_first_trubric\",\n",
    "    model_name=\"my_model\",\n",
    "    data_context_name=data_context.name,\n",
    "    data_context_version=data_context.version,\n",
    "    metadata={\"tag\": \"master\"},\n",
    "    validations=validations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e0cd3-8c54-459c-b7db-3f0134f6faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trubric to a local .json\n",
    "trubric.save_local(path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76277498-9ce8-4643-8c55-2cd4c313d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat my_first_trubric.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da79f0-0d21-4adc-b674-f56f09809a49",
   "metadata": {},
   "source": [
    "## Execute the trubric from the CLI tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964fa2fe-a6a1-4d20-b0b5-c85c2a2fd307",
   "metadata": {},
   "source": [
    "*Once you have saved a `Trubric`, run these validations on a new model & data context in your CI/CD/CT pipelines with the CLI tool. Read more about setting up the CLI [here](https://trubrics.github.io/trubrics-sdk/trubrics_cli/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15dd51f-624c-4786-a7df-d221f847b61d",
   "metadata": {},
   "source": [
    "Example code snippet:\n",
    "\n",
    "```bash\n",
    "(venv)$ trubrics run \\\n",
    "        --trubric-config-path \".\" \\\n",
    "        --trubric-output-file-path \".\" \\\n",
    "        --trubric-output-file-name \"my_new_trubric.json\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trubrics-venv",
   "language": "python",
   "name": "trubrics-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
